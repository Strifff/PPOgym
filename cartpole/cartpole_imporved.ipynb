{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole PPO Project\n",
    "\n",
    "## TODOs\n",
    "- [ ] **Dynamic Multi-Processing**: Implement dynamic multi-processing based on execution time.\n",
    "- [ ] **Atomic Functions**: Ensure atomic functions are efficiently used in the code.\n",
    "- [ ] **Checkpoints with Branching**: \n",
    "    - Implement checkpoints with branching logic.\n",
    "    - Select the best model if it outperforms the current one.\n",
    "- [ ] **Save Progression**: \n",
    "    - Visualize progression in the `README.md`.\n",
    "    - Incorporate meaningful progress data and visual indicators.\n",
    "- [ ] **Tensorboard Hyperparameter Tuning**: \n",
    "    - Set up Tensorboard for hyperparameter tuning and tracking performance.\n",
    "\n",
    "## Notations\n",
    "\n",
    "- **Steps > Episode > Epoch > Generation**\n",
    "    - The hierarchy of execution:\n",
    "        - Steps occur within an episode.\n",
    "        - Multiple episodes form an epoch.\n",
    "        - Generations are groups of epochs or models trained over time.\n",
    "\n",
    "## Roadmap\n",
    "- [ ] Fine-tune PPO hyperparameters for Cartpole.\n",
    "- [ ] Optimize environment reset and step procedures for speed.\n",
    "- [ ] Visualize performance and learning curve progression.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Build an efficient **PPO** agent for **Cartpole-v1** using vectorized environments.\n",
    "- Achieve high performance using checkpoints and branching techniques.\n",
    "- Track progress and optimize hyperparameters with **Tensorboard**.\n",
    "- Ensure best practices for multi-processing and atomic function handling.\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "- [ ] Experiment with various architectures for the PPO network.\n",
    "- [ ] Try more challenging environments after mastering Cartpole.\n",
    "- [ ] Expand branching checkpoints for longer-running tasks.\n",
    "\n",
    "## Notations\n",
    "\n",
    "- steps < episodes < iterations < epoch < generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "import ppo_network, ppo_wrapper, importlib\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetwork\n",
    "importlib.reload(ppo_wrapper) # Prevents caching issues with notebooks\n",
    "from ppo_wrapper import PPOWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole environment\n",
    "env_id = 'CartPole-v1'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 32\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: per vectorized env: 0.00 s\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "input_dims = 4\n",
    "output_dims = 2\n",
    "shared_hidden_dims = [512, 256, 128]\n",
    "policy_hidden_dims = [64, 64]\n",
    "value_hidden_dims = [64, 64]\n",
    "activation = nn.ReLU\n",
    "\n",
    "network_kwargs = {\n",
    "    'input_dims': input_dims,\n",
    "    'output_dims': output_dims,\n",
    "    'shared_hidden_dims': shared_hidden_dims,\n",
    "    'policy_hidden_dims': policy_hidden_dims,\n",
    "    'value_hidden_dims': value_hidden_dims,\n",
    "    'activation': activation,\n",
    "}\n",
    "\n",
    "network = PPONetwork(**network_kwargs)\n",
    "\n",
    "# Test forward pass\n",
    "now = time.time()\n",
    "for _ in range(100):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    policy, value = network(states_tensor)\n",
    "    \n",
    "    actions_dist = torch.distributions.Categorical(logits=policy)\n",
    "    actions = actions_dist.sample().numpy()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    #print(dones)\n",
    "\n",
    "print(f'Elapsed time: per vectorized env: {(time.time() - now)/num_envs:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 - Reward: 48.9375\n",
      "Generation 1 - Reward: 157.71875\n",
      "Generation 2 - Reward: 162.0625\n",
      "Generation 3 - Reward: 160.28125\n",
      "Generation 4 - Reward: 167.53125\n",
      "Generation 5 - Reward: 169.875\n",
      "Generation 6 - Reward: 155.71875\n",
      "Generation 7 - Reward: 162.84375\n",
      "Generation 8 - Reward: 185.125\n",
      "Generation 9 - Reward: 191.03125\n",
      "Generation 10 - Reward: 176.15625\n",
      "Generation 11 - Reward: 205.25\n",
      "Generation 12 - Reward: 444.3125\n",
      "Generation 13 - Reward: 767.9375\n",
      "Generation 14 - Reward: 908.5625\n",
      "Generation 15 - Reward: 1029.0\n",
      "Generation 16 - Reward: 1029.0\n",
      "Generation 17 - Reward: 1029.0\n",
      "Generation 18 - Reward: 1029.0\n",
      "Generation 19 - Reward: 1029.0\n",
      "Generation 20 - Reward: 1026.21875\n",
      "Generation 21 - Reward: 539.84375\n",
      "Generation 22 - Reward: 1029.0\n",
      "Generation 23 - Reward: 1029.0\n",
      "Generation 24 - Reward: 1029.0\n",
      "Generation 25 - Reward: 1029.0\n",
      "Generation 26 - Reward: 1029.0\n",
      "Generation 27 - Reward: 1029.0\n",
      "Generation 28 - Reward: 1029.0\n",
      "Generation 29 - Reward: 1029.0\n",
      "Generation 30 - Reward: 1029.0\n",
      "Generation 31 - Reward: 1029.0\n",
      "Generation 32 - Reward: 1029.0\n",
      "Generation 33 - Reward: 1029.0\n",
      "Generation 34 - Reward: 1029.0\n",
      "Generation 35 - Reward: 1029.0\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "final_lr = 1e-6\n",
    "gamma = 0.99\n",
    "lam = 0.90\n",
    "clip_eps = 0.2\n",
    "value_coef = 0.1\n",
    "entropy_coef = 0.05\n",
    "\n",
    "batch_size = 128\n",
    "batch_epochs = 5\n",
    "batch_shuffle = True\n",
    "\n",
    "iterations = 1024\n",
    "\n",
    "truncated_reward = 5\n",
    "\n",
    "debug_prints = False\n",
    "\n",
    "ppo_kwargs = {\n",
    "    'num_envs': num_envs,\n",
    "    'lr': lr,\n",
    "    'final_lr': final_lr,\n",
    "    'gamma': gamma,\n",
    "    'lam': lam,\n",
    "    'clip_eps': clip_eps,\n",
    "    'value_coef': value_coef,\n",
    "    'entropy_coef': entropy_coef,\n",
    "    'batch_size': batch_size,\n",
    "    'batch_epochs': batch_epochs,\n",
    "    'batch_shuffle': batch_shuffle,\n",
    "    'iterations': iterations,\n",
    "    'truncated_reward': truncated_reward,\n",
    "    'debug_prints': debug_prints,   \n",
    "}\n",
    "\n",
    "ppo_wrapper = PPOWrapper(envs_vector, network, **ppo_kwargs)\n",
    "\n",
    "ppo_wrapper.train(generations=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
