{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time, importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "\n",
    "import ppo_network\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetworkContinuous\n",
    "\n",
    "import ppo_wrapper\n",
    "importlib.reload(ppo_wrapper) # Prevents caching issues with notebooks\n",
    "from ppo_wrapper import PPOWrapper\n",
    "\n",
    "import hp_optimizer\n",
    "importlib.reload(hp_optimizer) # Prevents caching issues with notebooks\n",
    "from hp_optimizer import HPOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipedalWalker environment\n",
    "env_id = 'BipedalWalker-v3'\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy-Value Network\n",
    "# TODO - Move to PPO-kwargs\n",
    "input_dims = 24\n",
    "output_dims = 4\n",
    "\n",
    "shared_hidden_dims = [1024, 512, 256]\n",
    "shared_norm = nn.LayerNorm\n",
    "shared_activation = nn.SiLU\n",
    "\n",
    "mean_hidden_dims = [256, 128, 64]\n",
    "mean_norm = nn.LayerNorm\n",
    "mean_activation = nn.SiLU\n",
    "\n",
    "log_var_hidden_dims = [256, 128, 64]\n",
    "log_var_norm = nn.LayerNorm\n",
    "log_var_activation = nn.SiLU\n",
    "\n",
    "value_hidden_dims = [256, 128, 64]\n",
    "value_norm = nn.LayerNorm\n",
    "value_activation = nn.SiLU\n",
    "\n",
    "network_kwargs = {\n",
    "    'input_dims': input_dims,\n",
    "    'output_dims': output_dims,\n",
    "    \n",
    "    'shared_hidden_dims': shared_hidden_dims,\n",
    "    'shared_norm': shared_norm,\n",
    "    'shared_activation': shared_activation,\n",
    "    \n",
    "    'mean_hidden_dims': mean_hidden_dims,\n",
    "    'mean_norm': mean_norm,\n",
    "    'mean_activation': mean_activation,\n",
    "    \n",
    "    'log_var_hidden_dims': log_var_hidden_dims,\n",
    "    'log_var_norm': log_var_norm,\n",
    "    'log_var_activation': log_var_activation,\n",
    "    \n",
    "    'value_hidden_dims': value_hidden_dims,\n",
    "    'value_norm': value_norm,\n",
    "    'value_activation': value_activation,\n",
    "}\n",
    "\n",
    "network = PPONetworkContinuous(**network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsolhall/miniconda3/envs/PPOgym/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/axelsolhall/miniconda3/envs/PPOgym/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:253: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'torch.Tensor'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100.0 0.2304859267423526 -12.622472972945621\n",
      "-100.0 0.2873644568522795 -6.36015145400865\n",
      "-100.0 0.19427680503328523 -6.343521347976601\n",
      "-100.0 0.20793572314580164 -6.317216069847345\n",
      "-100.0 0.12989913108944895 -6.327480354468649\n",
      "-100.0 0.13981885327895363 -6.413346759195362\n",
      "-100.0 0.15931183652083197 -6.385776503688734\n",
      "-100.0 -0.0014435435930864257 -6.39929694516138\n",
      "-100.0 0.1446106155316047 -12.612024927697764\n",
      "-100.0 0.2925105965733528 -6.3645866462118965\n",
      "-100.0 0.038820299565792096 -6.3681835183574975\n",
      "-100.0 0.34818235574165735 -6.32815624402153\n",
      "-100.0 0.2098088111480065 -6.333597906532697\n",
      "-100.0 0.21618881428241726 -6.339153132613127\n",
      "-100.0 0.11594956006606302 -6.364745146981558\n",
      "-100.0 0.11598382264375687 -6.332459840530996\n",
      "-100.0 0.05749896252155304 -6.321506784322516\n",
      "-100.0 0.0444431150754281 -6.326169084674213\n",
      "-100.0 0.14825553478797515 -6.31801773931152\n",
      "-100.0 0.012553291082382201 -6.341946622476447\n",
      "-100.0 0.04517381640275201 -6.364416866095679\n",
      "-100.0 0.09939028012752534 -6.341665229038607\n",
      "-100.0 0.03141283345222474 -6.332565505555986\n",
      "-100.0 0.09612407234311104 -6.3351798883381605\n",
      "-100.0 0.11978005752960605 -6.283114415814576\n",
      "-100.0 0.09360261102517328 -6.356651852237139\n",
      "-100.0 0.0561531532307454 -6.359764861966207\n",
      "-100.0 0.12472004701694091 -6.404852029025866\n",
      "-100.0 0.1291429790655766 -6.345477538479337\n",
      "-100.0 0.12359334470828257 -6.345823483811381\n",
      "-100.0 0.24354497349262236 -6.313206031535443\n",
      "-100.0 0.14998217565814417 -6.283558739864112\n",
      "-100.0 0.08609540488322812 -6.346088750060569\n",
      "-100.0 0.06738971432049952 -6.360719371348619\n",
      "-100.0 0.13170053865512452 -6.325020675876934\n",
      "-100.0 0.0393942266019694 -6.344850849920807\n",
      "-100.0 0.03835338362057886 -6.331593971705065\n",
      "-100.0 0.11098535330097 -6.306549732675156\n",
      "-100.0 0.1112990014155694 -6.326663792990148\n",
      "-100.0 0.17331931976477422 -6.2745298101976505\n",
      "Elapsed time: per vectorized env: 0.64 s\n"
     ]
    }
   ],
   "source": [
    "# Test forward passes\n",
    "now = time.time()\n",
    "for _ in range(1000):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    mean, log_var, value = network(states_tensor)\n",
    "    std_dev = torch.exp(log_var / 2)\n",
    "    \n",
    "    actions_dist = torch.distributions.Normal(mean, std_dev)\n",
    "    actions = actions_dist.sample()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    if dones.any():\n",
    "        print(min(rewards), max(rewards), rewards.mean())\n",
    "print(\n",
    "    f'Elapsed time: per vectorized env: {(time.time() - now)/num_envs:.2f} s'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
