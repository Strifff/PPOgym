{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "import ppo_network, ppo_wrapper, importlib\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetwork\n",
    "importlib.reload(ppo_wrapper) # Prevents caching issues with notebooks\n",
    "from ppo_wrapper import PPOWrapper\n",
    "\n",
    "import hp_optimizer\n",
    "importlib.reload(hp_optimizer) # Prevents caching issues with notebooks\n",
    "from hp_optimizer import HPOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LunarLander environment\n",
    "env_id = 'LunarLander-v2'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: per vectorized env: 0.01 s\n"
     ]
    }
   ],
   "source": [
    "# Policy-Value Network\n",
    "input_dims = 8\n",
    "output_dims = 4\n",
    "shared_hidden_dims = [512, 256, 128]\n",
    "shared_norm = nn.LayerNorm\n",
    "policy_hidden_dims = [128, 64]\n",
    "policy_norm = nn.LayerNorm\n",
    "value_hidden_dims = [128, 64]\n",
    "value_norm = nn.LayerNorm\n",
    "activation = nn.ReLU\n",
    "\n",
    "network_kwargs = {\n",
    "    'input_dims': input_dims,\n",
    "    'output_dims': output_dims,\n",
    "    'shared_hidden_dims': shared_hidden_dims,\n",
    "    'shared_norm': shared_norm,\n",
    "    'policy_hidden_dims': policy_hidden_dims,\n",
    "    'policy_norm': policy_norm,\n",
    "    'value_hidden_dims': value_hidden_dims,\n",
    "    'value_norm': value_norm,\n",
    "    'activation': activation,\n",
    "}\n",
    "\n",
    "network = PPONetwork(**network_kwargs)\n",
    "\n",
    "# Test forward pass\n",
    "now = time.time()\n",
    "for _ in range(100):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    policy, value = network(states_tensor)\n",
    "    \n",
    "    actions_dist = torch.distributions.Categorical(logits=policy)\n",
    "    actions = actions_dist.sample().numpy()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    #print(dones)\n",
    "\n",
    "print(\n",
    "    f'Elapsed time: per vectorized env: {(time.time() - now)/num_envs:.2f} s'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Wrapper\n",
    "lr = 3e-4\n",
    "final_lr = 1e-6\n",
    "\n",
    "gamma = 0.99\n",
    "lam = 0.975\n",
    "\n",
    "clip_eps = 0.25\n",
    "final_clip_eps = 0.01\n",
    "\n",
    "value_coef = 0.7\n",
    "\n",
    "entropy_coef = 0.1\n",
    "final_entropy_coef = 0.01\n",
    "\n",
    "batch_size = 256\n",
    "batch_epochs = 5\n",
    "batch_shuffle = True\n",
    "\n",
    "iterations = 2048\n",
    "\n",
    "truncated_reward = -300\n",
    "\n",
    "debug_prints = False\n",
    "\n",
    "ppo_kwargs = {\n",
    "    'num_envs': num_envs,\n",
    "    'lr': lr,\n",
    "    'final_lr': final_lr,\n",
    "    'gamma': gamma,\n",
    "    'lam': lam,\n",
    "    'clip_eps': clip_eps,\n",
    "    'final_clip_eps': final_clip_eps,\n",
    "    'value_coef': value_coef,\n",
    "    'entropy_coef': entropy_coef,\n",
    "    'final_entropy_coef': final_entropy_coef,\n",
    "    'batch_size': batch_size,\n",
    "    'batch_epochs': batch_epochs,\n",
    "    'batch_shuffle': batch_shuffle,\n",
    "    'iterations': iterations,\n",
    "    'truncated_reward': truncated_reward,\n",
    "    'debug_prints': debug_prints,   \n",
    "}\n",
    "\n",
    "ppo_wrapper = PPOWrapper(envs_vector, network, **ppo_kwargs)\n",
    "\n",
    "#ppo_wrapper.train(generations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evolution with save generations: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "Generation 0\t- Reward: -1082.34,\tw/o trunc.: -1082.34\n",
      "Generation 1\t- Reward: -716.93,\tw/o trunc.: -716.93\n",
      "Generation 2\t- Reward: -1242.10,\tw/o trunc.: -1242.10\n",
      "Generation 3\t- Reward: -492.57,\tw/o trunc.: -248.82\n",
      "Generation 4\t- Reward: -349.24,\tw/o trunc.: -86.74\n",
      "Generation 5\t- Reward: -216.67,\tw/o trunc.: -122.92\n",
      "Generation 6\t- Reward: -371.95,\tw/o trunc.: -90.70\n",
      "Generation 7\t- Reward: -478.63,\tw/o trunc.: -216.13\n",
      "Generation 8\t- Reward: -177.92,\tw/o trunc.: -27.92\n",
      "Generation 9\t- Reward: 12.10,\tw/o trunc.: 49.60\n",
      "Generation 10\t- Reward: -60.55,\tw/o trunc.: 70.70\n",
      "Generation 11\t- Reward: 77.50,\tw/o trunc.: 152.50\n",
      "Generation 12\t- Reward: -89.02,\tw/o trunc.: 60.98\n",
      "Generation 13\t- Reward: -119.54,\tw/o trunc.: -25.79\n",
      "Generation 14\t- Reward: 84.73,\tw/o trunc.: 103.48\n",
      "Generation 15\t- Reward: -35.03,\tw/o trunc.: 21.22\n",
      "Generation 16\t- Reward: 68.65,\tw/o trunc.: 106.15\n",
      "Generation 17\t- Reward: -2.12,\tw/o trunc.: -2.12\n",
      "Generation 18\t- Reward: -32.73,\tw/o trunc.: -32.73\n",
      "Generation 19\t- Reward: -55.27,\tw/o trunc.: -36.52\n",
      "Generation 20\t- Reward: -69.23,\tw/o trunc.: -69.23\n",
      "Generation 21\t- Reward: 65.56,\tw/o trunc.: 84.31\n",
      "Generation 22\t- Reward: 113.32,\tw/o trunc.: 113.32\n",
      "Generation 23\t- Reward: 60.46,\tw/o trunc.: 97.96\n",
      "Generation 24\t- Reward: 70.47,\tw/o trunc.: 70.47\n",
      "Generation 25\t- Reward: 139.41,\tw/o trunc.: 139.41\n",
      "Generation 26\t- Reward: 138.75,\tw/o trunc.: 176.25\n",
      "Generation 27\t- Reward: 122.13,\tw/o trunc.: 140.88\n",
      "Generation 28\t- Reward: 119.29,\tw/o trunc.: 119.29\n",
      "Generation 29\t- Reward: 104.03,\tw/o trunc.: 104.03\n",
      "Generation 30\t- Reward: 108.77,\tw/o trunc.: 108.77\n",
      "Generation 31\t- Reward: 30.17,\tw/o trunc.: 30.17\n",
      "Generation 32\t- Reward: 84.97,\tw/o trunc.: 103.72\n",
      "Generation 33\t- Reward: 157.07,\tw/o trunc.: 157.07\n",
      "Generation 34\t- Reward: 85.79,\tw/o trunc.: 123.29\n",
      "Generation 35\t- Reward: -129.86,\tw/o trunc.: 1.39\n",
      "Generation 36\t- Reward: 94.49,\tw/o trunc.: 150.74\n",
      "Generation 37\t- Reward: 174.06,\tw/o trunc.: 192.81\n",
      "Generation 38\t- Reward: 173.77,\tw/o trunc.: 173.77\n",
      "Generation 39\t- Reward: 206.38,\tw/o trunc.: 225.13\n",
      "Generation 40\t- Reward: 146.81,\tw/o trunc.: 203.06\n",
      "Generation 41\t- Reward: 112.74,\tw/o trunc.: 168.99\n",
      "Generation 42\t- Reward: 176.62,\tw/o trunc.: 214.12\n",
      "Generation 43\t- Reward: 31.90,\tw/o trunc.: 163.15\n",
      "Generation 44\t- Reward: 28.20,\tw/o trunc.: 121.95\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "hp_optimizer = HPOptimizer(\n",
    "    env_kwargs=env_kwargs,\n",
    "    num_envs=num_envs,\n",
    "    network_class = PPONetwork,\n",
    "    network_kwargs=network_kwargs,\n",
    "    ppo_class=PPOWrapper,\n",
    "    ppo_kwargs=ppo_kwargs,\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    'clip_eps', \n",
    "    'value_coef', \n",
    "    'entropy_coef', \n",
    "    'batch_size', \n",
    "    'batch_epochs',\n",
    "    ]\n",
    "\n",
    "# evolutions = hp_optimizer.optimize_hyperparameters(\n",
    "#      parameters, generations=250, num_trials = 8,\n",
    "#      )\n",
    "\n",
    "hp_optimizer.evolution_video(\n",
    "    generations=100, video_folder = 'videos', increments=10, max_frames=max_episode_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
